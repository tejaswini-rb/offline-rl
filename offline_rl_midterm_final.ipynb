{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "!apt install -y libgl1-mesa-glx libosmesa6 libglfw3 patchelf\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import copy\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dOynhsNlfuP",
        "outputId": "046a5fb1-1d2e-48bd-9dee-1c34de7e6c1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m726.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgles1 libvulkan1\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx libglfw3 libosmesa6 patchelf\n",
            "0 upgraded, 4 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 3,276 kB of archives.\n",
            "After this operation, 13.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglfw3 amd64 3.3.6-1 [83.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.3 [3,115 kB]\n",
            "Fetched 3,276 kB in 1s (3,844 kB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 125044 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libglfw3:amd64.\n",
            "Preparing to unpack .../libglfw3_3.3.6-1_amd64.deb ...\n",
            "Unpacking libglfw3:amd64 (3.3.6-1) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../libosmesa6_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglfw3:amd64 (3.3.6-1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualization code. Used later.\n",
        "import os\n",
        "from gym.wrappers import RecordVideo\n",
        "from IPython.display import Video, display, clear_output\n",
        "\n",
        "# Force MuJoCo to use EGL for rendering (important for Colab)\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "\n",
        "def visualize(agent):\n",
        "    \"\"\"Visualize agent with a custom camera angle.\"\"\"\n",
        "\n",
        "    # Create environment in rgb_array mode\n",
        "    env = gym.make(\"HalfCheetah-v5\", render_mode=\"rgb_array\", reset_noise_scale=0.2)\n",
        "\n",
        "    # Apply video recording wrapper\n",
        "    env = RecordVideo(env, video_folder=\"./\", episode_trigger=lambda x: True)\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    # Access the viewer object through mujoco_py\n",
        "    viewer = env.unwrapped.mujoco_renderer.viewer  # Access viewer\n",
        "    viewer.cam.distance = 3.0     # Set camera distance\n",
        "    viewer.cam.azimuth = 90       # Rotate camera around pendulum\n",
        "    viewer.cam.elevation = 0   # Tilt the camera up/down\n",
        "\n",
        "\n",
        "    for t in range(512):\n",
        "        with torch.no_grad():\n",
        "            actions = agent.get_action(obs)\n",
        "        obs, _, done, _ = env.step(actions)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.render()\n",
        "    env.close()\n",
        "\n",
        "    # Display the latest video\n",
        "    clear_output(wait=True)\n",
        "    display(Video(\"./rl-video-episode-0.mp4\", embed=True))\n"
      ],
      "metadata": {
        "id": "ZfeO7A9nnh6N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "\n",
        "        self.buffer[self.position] = (\n",
        "            torch.FloatTensor(state).to(self.device),\n",
        "            torch.FloatTensor(action).to(self.device),\n",
        "            torch.FloatTensor([reward]).to(self.device),\n",
        "            torch.FloatTensor(next_state).to(self.device),\n",
        "            torch.FloatTensor([done]).to(self.device),\n",
        "        )\n",
        "\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            torch.stack(state).to(self.device),\n",
        "            torch.stack(action).to(self.device),\n",
        "            torch.stack(reward).to(self.device),\n",
        "            torch.stack(next_state).to(self.device),\n",
        "            torch.stack(done).to(self.device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "qYVtxGJ8mBtA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SvzuI1LilU01"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "# Q-Network Definition\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        # Concatenate state and action, then produce a single Q-value\n",
        "        return self.fc(torch.cat([state, action], dim=1))\n",
        "\n",
        "# Policy Network Definition\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Returns actions in [-1, 1]\n",
        "        return 1.0 * torch.tanh(self.fc(state))\n",
        "\n",
        "# CQL Agent (twin Q-networks, random+policy actions in CQL penalty)\n",
        "class CQLAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        lr=3e-4,\n",
        "        gamma=0.99,\n",
        "        tau=0.005,\n",
        "        alpha=3.0,\n",
        "        num_random_actions=10\n",
        "    ):\n",
        "        # Detect GPU or CPU\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "        # CQL penalty coefficient\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Number of random actions for the conservative penalty\n",
        "        self.num_random_actions = num_random_actions\n",
        "\n",
        "        # -------------------------------\n",
        "        #  Create twin Q networks\n",
        "        # -------------------------------\n",
        "        self.q1 = QNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.q2 = QNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.target_q1 = copy.deepcopy(self.q1).to(self.device)\n",
        "        self.target_q2 = copy.deepcopy(self.q2).to(self.device)\n",
        "\n",
        "        # For backward compatibility with main.py calls:\n",
        "        self.q_net = self.q1\n",
        "        self.target_q_net = self.target_q1\n",
        "\n",
        "        # Create policy network\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.target_policy = copy.deepcopy(self.policy).to(self.device)\n",
        "\n",
        "        # Initialize optimizers\n",
        "        self.q1_optim = optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        self.q2_optim = optim.Adam(self.q2.parameters(), lr=lr)\n",
        "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=lr / 10)\n",
        "\n",
        "        # Logging\n",
        "        self.q_loss = 0.0\n",
        "        self.policy_loss = 0.0\n",
        "        self.cql_loss = 0.0  # We'll store sum of CQL penalties from Q1 and Q2\n",
        "\n",
        "    def get_action(self, state, deterministic=False):\n",
        "        \"\"\"Select action from current policy; add noise if not deterministic.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Convert to Tensor on self.device\n",
        "            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            action_t = self.policy(state_t)\n",
        "\n",
        "            if not deterministic:\n",
        "                noise = 0.1 * torch.randn_like(action_t).to(self.device)\n",
        "                action_t = torch.clamp(action_t + noise, -1.0, 1.0)\n",
        "\n",
        "            # Return numpy array on CPU\n",
        "            return action_t.squeeze(0).cpu().numpy()\n",
        "\n",
        "    def get_q_loss(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"\n",
        "        Q loss for both networks:\n",
        "          L_Q = MSE(Q1 - target) + MSE(Q2 - target)\n",
        "                + alpha * (CQL_penalty1 + CQL_penalty2).\n",
        "\n",
        "        Each \"CQL_penalty\" is:\n",
        "          E[logsumexp(Q(s,a)) - Q(s,a_in_batch)],\n",
        "        sampling random + policy actions for the logsumexp.\n",
        "        \"\"\"\n",
        "        # Move to device\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "\n",
        "        # -------------------------------------------\n",
        "        # Standard Bellman backups\n",
        "        # -------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.target_policy(next_states)\n",
        "            next_q1 = self.target_q1(next_states, next_actions)\n",
        "            next_q2 = self.target_q2(next_states, next_actions)\n",
        "            # Use min for the target\n",
        "            target_q = rewards + self.gamma * torch.min(next_q1, next_q2) * (1 - dones)\n",
        "\n",
        "        q1_vals = self.q1(states, actions)\n",
        "        q2_vals = self.q2(states, actions)\n",
        "\n",
        "        bellman_loss = F.mse_loss(q1_vals, target_q) + F.mse_loss(q2_vals, target_q)\n",
        "\n",
        "        # -------------------------------------------\n",
        "        # CQL: sample random + policy actions\n",
        "        # -------------------------------------------\n",
        "        batch_size = states.shape[0]\n",
        "\n",
        "        # 1) Random actions in [-1, 1]\n",
        "        random_actions = torch.FloatTensor(\n",
        "            np.random.uniform(-1, 1, (batch_size, self.num_random_actions, self.action_dim))\n",
        "        ).to(self.device)\n",
        "\n",
        "        # 2) Current policy actions\n",
        "        with torch.no_grad():\n",
        "            policy_actions = self.policy(states)  # shape (B, action_dim)\n",
        "\n",
        "        # Combine random + policy => shape (B, N+1, a_dim)\n",
        "        policy_actions = policy_actions.unsqueeze(1)\n",
        "        all_actions = torch.cat([random_actions, policy_actions], dim=1)\n",
        "\n",
        "\n",
        "        q1_vals_all = []\n",
        "        q2_vals_all = []\n",
        "        for i in range(all_actions.shape[1]):\n",
        "            q1_i = self.q1(states, all_actions[:, i, :])\n",
        "            q2_i = self.q2(states, all_actions[:, i, :])\n",
        "            q1_vals_all.append(q1_i)\n",
        "            q2_vals_all.append(q2_i)\n",
        "\n",
        "        q1_vals_all = torch.cat(q1_vals_all, dim=1)  # shape (B, N+1)\n",
        "        q2_vals_all = torch.cat(q2_vals_all, dim=1)  # shape (B, N+1)\n",
        "\n",
        "        # logsumexp across those actions (then subtract log(# actions))\n",
        "        logsumexp_q1 = torch.logsumexp(q1_vals_all, dim=1, keepdim=True) - np.log(q1_vals_all.shape[1])\n",
        "        logsumexp_q2 = torch.logsumexp(q2_vals_all, dim=1, keepdim=True) - np.log(q2_vals_all.shape[1])\n",
        "\n",
        "        # CQL penalty for Q1 and Q2\n",
        "        cql_penalty1 = (logsumexp_q1 - q1_vals).mean()\n",
        "        cql_penalty2 = (logsumexp_q2 - q2_vals).mean()\n",
        "        cql_penalty = cql_penalty1 + cql_penalty2\n",
        "\n",
        "        total_q_loss = bellman_loss + self.alpha * cql_penalty\n",
        "\n",
        "        # Logging\n",
        "        self.cql_loss = cql_penalty.item()\n",
        "        return total_q_loss\n",
        "\n",
        "    def get_policy_loss(self, states):\n",
        "        \"\"\"Policy update: minimize -E[min(Q1, Q2)].\"\"\"\n",
        "        states = states.to(self.device)\n",
        "        actions_pi = self.policy(states)\n",
        "        q1_pi = self.q1(states, actions_pi)\n",
        "        q2_pi = self.q2(states, actions_pi)\n",
        "        q_min = torch.min(q1_pi, q2_pi)\n",
        "        policy_loss = -q_min.mean()\n",
        "        return policy_loss\n",
        "\n",
        "    def update(self, replay_buffer, batch_size=256):\n",
        "        \"\"\"Sample from replay buffer and update Q networks + policy.\"\"\"\n",
        "        if len(replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "        # 1) Update Q-functions\n",
        "        self.q1_optim.zero_grad()\n",
        "        self.q2_optim.zero_grad()\n",
        "        q_loss = self.get_q_loss(states, actions, rewards, next_states, dones)\n",
        "        q_loss.backward()\n",
        "        self.q1_optim.step()\n",
        "        self.q2_optim.step()\n",
        "        self.q_loss = q_loss.item()\n",
        "\n",
        "        # 2) Update Policy\n",
        "        self.policy_optim.zero_grad()\n",
        "        policy_loss = self.get_policy_loss(states)\n",
        "        policy_loss.backward()\n",
        "        self.policy_optim.step()\n",
        "        self.policy_loss = policy_loss.item()\n",
        "\n",
        "        # 3) Soft-update target networks\n",
        "        with torch.no_grad():\n",
        "            for param, target_param in zip(self.q1.parameters(), self.target_q1.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for param, target_param in zip(self.q2.parameters(), self.target_q2.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for param, target_param in zip(self.policy.parameters(), self.target_policy.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(agent, env, num_eval_episodes=5):\n",
        "    \"\"\"\n",
        "    Runs 'num_eval_episodes' episodes in the given 'env' using the agent's\n",
        "    policy. Returns the average total reward across these episodes.\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    for ep in range(num_eval_episodes):\n",
        "        # Reset environment\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "\n",
        "        while not done:\n",
        "            # Get the action from the agent's policy, in deterministic mode\n",
        "            action = agent.get_action(state, deterministic=True)\n",
        "\n",
        "            # Step the environment with that action\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            ep_return += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        returns.append(ep_return)\n",
        "    return np.mean(returns)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "ONLINE_EPISODES = 250   # how many episodes to collect online\n",
        "MAX_STEPS = 1000\n",
        "BUFFER_SIZE = 100000\n",
        "LOG_INTERVAL = 10\n",
        "REWARD_MULTIPLIER = 1\n",
        "LEARNING_RATE = 3e-4  # Match the CQL model's LR\n",
        "RANDOM_EPISODES = 30  # how many random episodes to add\n",
        "OFFLINE_TRAIN_STEPS = 1000  # how many offline training iterations\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make(\"InvertedPendulum-v5\", max_episode_steps=MAX_STEPS)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# TensorBoard logging\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Initialize CQL agent and replay buffer\n",
        "agent = CQLAgent(state_dim, action_dim, lr=LEARNING_RATE, alpha=0.0)\n",
        "replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
        "\n",
        "# Tracking losses and metrics\n",
        "policy_losses = []\n",
        "q_losses = []\n",
        "cql_losses = []\n",
        "cumulative_rewards = []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# (A) Short Online Training Phase\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"=== Collecting Online Episodes & Updating Agent Online ===\")\n",
        "evaluation_returns = []\n",
        "\n",
        "# Online Training Loop\n",
        "for ep in range(ONLINE_EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    ep_return = 0.0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done and step_count < MAX_STEPS:\n",
        "        action = agent.get_action(state, deterministic=False)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        ep_return += reward\n",
        "        step_count += 1\n",
        "\n",
        "        # Store in replay buffer\n",
        "        replay_buffer.add(state, action, reward * REWARD_MULTIPLIER, next_state, float(done))\n",
        "        state = next_state\n",
        "\n",
        "        # Online training update\n",
        "        agent.update(replay_buffer, BATCH_SIZE)\n",
        "\n",
        "    # Evaluate every few episodes\n",
        "    if ep % 2 == 0:\n",
        "        eval_ret = evaluate_policy(agent, env, num_eval_episodes=2)\n",
        "        evaluation_returns.append(eval_ret)  # Track evaluation returns\n",
        "        print(f\"Online Episode {ep}, Return={ep_return:.1f}, Eval={eval_ret:.1f}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# (B) Add random data to the buffer (optional)\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n=== Collecting Random Episodes ===\")\n",
        "for _ in range(RANDOM_EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        replay_buffer.add(\n",
        "            state,\n",
        "            action,\n",
        "            reward * REWARD_MULTIPLIER,\n",
        "            next_state,\n",
        "            float(done)\n",
        "        )\n",
        "        state = next_state\n",
        "\n",
        "print(f\"Replay buffer size after random data: {len(replay_buffer)}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# (C) Offline Training Loop (NO new environment interaction)\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n=== Offline Training Only ===\")\n",
        "# Offline Training Loop\n",
        "for step in range(OFFLINE_TRAIN_STEPS):\n",
        "    agent.update(replay_buffer, BATCH_SIZE)\n",
        "\n",
        "    # Track batch reward sum\n",
        "    if len(replay_buffer) >= BATCH_SIZE:\n",
        "        states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
        "        batch_reward_sum = rewards.sum().item()\n",
        "    else:\n",
        "        batch_reward_sum = 0.0\n",
        "\n",
        "    q_losses.append(agent.q_loss)\n",
        "    policy_losses.append(agent.policy_loss)\n",
        "    cql_losses.append(agent.cql_loss)\n",
        "    cumulative_rewards.append(batch_reward_sum)\n",
        "\n",
        "    # Evaluate agent performance every LOG_INTERVAL steps\n",
        "    if step % LOG_INTERVAL == 0:\n",
        "        eval_ret = evaluate_policy(agent, env, num_eval_episodes=5)\n",
        "        evaluation_returns.append(eval_ret)  # Track evaluation returns\n",
        "        print(f\"Offline Step {step}, Eval Return: {eval_ret:.2f}\")\n",
        "\n",
        "    # Logging to TensorBoard\n",
        "    writer.add_scalar(\"Reward/BatchSum\", batch_reward_sum, step)\n",
        "    writer.add_scalar(\"Loss/Q\", agent.q_loss, step)\n",
        "    writer.add_scalar(\"Loss/Policy\", agent.policy_loss, step)\n",
        "    writer.add_scalar(\"Loss/CQL\", agent.cql_loss, step)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Final Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "eval_episodes = 5\n",
        "avg_return = evaluate_policy(agent, env, num_eval_episodes=eval_episodes)\n",
        "print(f\"\\nEvaluated policy over {eval_episodes} episodes. Average return: {avg_return:.2f}\")\n",
        "\n",
        "# Plot training metrics\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(q_losses, label=\"Q Loss\")\n",
        "plt.plot(policy_losses, label=\"Policy Loss\")\n",
        "if cql_losses:\n",
        "    plt.plot(cql_losses, label=\"CQL Loss\")\n",
        "plt.xlabel(\"Offline Update Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot evaluation returns over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(evaluation_returns, label=\"Evaluation Return\")\n",
        "plt.xlabel(\"Evaluation Step (Online + Offline)\")\n",
        "plt.ylabel(\"Average Return\")\n",
        "plt.title(\"Evaluation Return Over Time\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cleanup\n",
        "writer.close()\n",
        "env.close()\n",
        "\n",
        "'''\n",
        "def evaluate_policy(agent, env, num_eval_episodes=5):\n",
        "    \"\"\"\n",
        "    Runs 'num_eval_episodes' episodes in the given 'env' using the agent's\n",
        "    policy. Returns the average total reward across these episodes.\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    for ep in range(num_eval_episodes):\n",
        "        # Reset environment\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "\n",
        "        while not done:\n",
        "            # Get the action from the agent's policy, in deterministic mode\n",
        "            action = agent.get_action(state, deterministic=True)\n",
        "\n",
        "            # Step the environment with that action\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            ep_return += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        returns.append(ep_return)\n",
        "    return np.mean(returns)\n",
        "\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "MAX_EPISODES = 300\n",
        "MAX_STEPS = 1000\n",
        "BUFFER_SIZE = 100000\n",
        "LOG_INTERVAL = 10\n",
        "REWARD_MULTIPLIER = 1\n",
        "LEARNING_RATE = 3e-4  # Match the CQL model's LR\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make(\"HalfCheetah-v5\", max_episode_steps=MAX_STEPS)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "# Setup device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize logging and agent\n",
        "writer = SummaryWriter()\n",
        "agent = CQLAgent(state_dim, action_dim, lr=LEARNING_RATE, alpha=1.0)\n",
        "\n",
        "# Move agent model components to GPU\n",
        "# agent.q1.to(device)\n",
        "# agent.target_q_net.to(device)\n",
        "# agent.policy.to(device)\n",
        "# agent.target_policy.to(device)\n",
        "\n",
        "# Initialize replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_SIZE)  # No device needed in the replay buffer\n",
        "\n",
        "# Tracking losses and metrics\n",
        "policy_losses = []\n",
        "q_losses = []\n",
        "cql_losses = []\n",
        "episode_lengths = []\n",
        "cumulative_rewards = []\n",
        "\n",
        "for _ in range(500):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        reward_to_go = reward * REWARD_MULTIPLIER\n",
        "        print(reward_to_go)\n",
        "        done = terminated or truncated\n",
        "        replay_buffer.add(state, action, reward_to_go, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "# Training loop\n",
        "for episode in range(MAX_EPISODES):\n",
        "    episode_reward = 0\n",
        "\n",
        "    if len(replay_buffer) > BATCH_SIZE:\n",
        "        batch = replay_buffer.sample(BATCH_SIZE)\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "        agent.update(replay_buffer, BATCH_SIZE)\n",
        "        # 3) Log the sum of rewards in this batch (not necessarily a real 'episode' reward)\n",
        "        episode_reward = rewards.sum().item()\n",
        "\n",
        "        # Collect the float losses from the agent\n",
        "        q_losses.append(agent.q_loss)\n",
        "        policy_losses.append(agent.policy_loss)\n",
        "        cql_losses.append(agent.cql_loss)\n",
        "\n",
        "        # Print debug info\n",
        "        print(f\"Episode {episode}, Batch Reward Sum: {episode_reward:.2f}\")\n",
        "        print(f\"  Q-Loss: {agent.q_loss:.4f}, Policy Loss: {agent.policy_loss:.4f}, CQL Loss: {agent.cql_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # Track episode length and cumulative reward\n",
        "    cumulative_rewards.append(episode_reward)\n",
        "\n",
        "    # Logging to TensorBoard\n",
        "    if episode % LOG_INTERVAL == 0:\n",
        "        writer.add_scalar(\"Reward/Episode\", episode_reward, episode)\n",
        "        if q_losses:\n",
        "            writer.add_scalar(\"Loss/Q\", q_losses[-1], episode)\n",
        "        if policy_losses:\n",
        "            writer.add_scalar(\"Loss/Policy\", policy_losses[-1], episode)\n",
        "        if cql_losses:\n",
        "            writer.add_scalar(\"Loss/CQL\", cql_losses[-1], episode)\n",
        "\n",
        "eval_episodes = 5\n",
        "avg_return = evaluate_policy(agent, env, num_eval_episodes=eval_episodes)\n",
        "print(f\"\\nEvaluated policy over {eval_episodes} episodes. Average return: {avg_return:.2f}\")\n",
        "\n",
        "# Plot training metrics\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(q_losses, label=\"Q Loss\")\n",
        "plt.plot(policy_losses, label=\"Policy Loss\")\n",
        "if cql_losses:\n",
        "    plt.plot(cql_losses, label=\"CQL Loss\")\n",
        "plt.xlabel(\"Update Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot cumulative rewards\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(cumulative_rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Cumulative Reward\")\n",
        "plt.title(\"Training Rewards\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cleanup\n",
        "writer.close()\n",
        "env.close()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcfxWpIzlXoF",
        "outputId": "131ae917-da5d-4766-d838-f7ede5a49fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "=== Collecting Online Episodes & Updating Agent Online ===\n",
            "Online Episode 0, Return=9.0, Eval=9.0\n",
            "Online Episode 2, Return=11.0, Eval=9.5\n",
            "Online Episode 4, Return=9.0, Eval=9.5\n",
            "Online Episode 6, Return=12.0, Eval=10.0\n",
            "Online Episode 8, Return=9.0, Eval=10.0\n",
            "Online Episode 10, Return=10.0, Eval=9.5\n",
            "Online Episode 12, Return=11.0, Eval=11.0\n",
            "Online Episode 14, Return=9.0, Eval=8.0\n",
            "Online Episode 16, Return=8.0, Eval=6.5\n",
            "Online Episode 18, Return=6.0, Eval=6.0\n",
            "Online Episode 20, Return=4.0, Eval=5.0\n",
            "Online Episode 22, Return=5.0, Eval=5.0\n",
            "Online Episode 24, Return=4.0, Eval=5.0\n",
            "Online Episode 26, Return=4.0, Eval=4.0\n",
            "Online Episode 28, Return=5.0, Eval=4.0\n",
            "Online Episode 30, Return=4.0, Eval=4.0\n",
            "Online Episode 32, Return=4.0, Eval=4.0\n",
            "Online Episode 34, Return=4.0, Eval=4.0\n",
            "Online Episode 36, Return=4.0, Eval=4.0\n",
            "Online Episode 38, Return=4.0, Eval=4.0\n",
            "Online Episode 40, Return=4.0, Eval=4.0\n",
            "Online Episode 42, Return=4.0, Eval=4.0\n",
            "Online Episode 44, Return=4.0, Eval=4.0\n",
            "Online Episode 46, Return=4.0, Eval=4.0\n",
            "Online Episode 48, Return=4.0, Eval=4.0\n",
            "Online Episode 50, Return=4.0, Eval=4.0\n",
            "Online Episode 52, Return=4.0, Eval=4.0\n",
            "Online Episode 54, Return=4.0, Eval=4.0\n",
            "Online Episode 56, Return=4.0, Eval=4.0\n",
            "Online Episode 58, Return=4.0, Eval=4.0\n",
            "Online Episode 60, Return=4.0, Eval=4.0\n",
            "Online Episode 62, Return=4.0, Eval=4.0\n",
            "Online Episode 64, Return=4.0, Eval=4.5\n",
            "Online Episode 66, Return=4.0, Eval=5.0\n",
            "Online Episode 68, Return=5.0, Eval=5.0\n",
            "Online Episode 70, Return=6.0, Eval=5.0\n",
            "Online Episode 72, Return=5.0, Eval=6.0\n",
            "Online Episode 74, Return=6.0, Eval=7.0\n",
            "Online Episode 76, Return=7.0, Eval=7.0\n",
            "Online Episode 78, Return=6.0, Eval=8.0\n",
            "Online Episode 80, Return=9.0, Eval=9.0\n",
            "Online Episode 82, Return=11.0, Eval=26.0\n",
            "Online Episode 84, Return=13.0, Eval=12.5\n",
            "Online Episode 86, Return=11.0, Eval=9.0\n",
            "Online Episode 88, Return=9.0, Eval=9.0\n",
            "Online Episode 90, Return=13.0, Eval=14.0\n",
            "Online Episode 92, Return=13.0, Eval=14.0\n",
            "Online Episode 94, Return=12.0, Eval=13.0\n",
            "Online Episode 96, Return=11.0, Eval=10.5\n",
            "Online Episode 98, Return=12.0, Eval=11.0\n",
            "Online Episode 100, Return=13.0, Eval=10.5\n",
            "Online Episode 102, Return=10.0, Eval=10.5\n",
            "Online Episode 104, Return=12.0, Eval=11.5\n",
            "Online Episode 106, Return=12.0, Eval=11.5\n",
            "Online Episode 108, Return=14.0, Eval=12.0\n",
            "Online Episode 110, Return=12.0, Eval=12.5\n",
            "Online Episode 112, Return=14.0, Eval=14.5\n",
            "Online Episode 114, Return=13.0, Eval=18.0\n",
            "Online Episode 116, Return=16.0, Eval=16.0\n",
            "Online Episode 118, Return=22.0, Eval=19.5\n",
            "Online Episode 120, Return=30.0, Eval=26.5\n",
            "Online Episode 122, Return=37.0, Eval=34.5\n",
            "Online Episode 124, Return=69.0, Eval=73.0\n",
            "Online Episode 126, Return=42.0, Eval=55.0\n",
            "Online Episode 128, Return=42.0, Eval=43.5\n",
            "Online Episode 130, Return=54.0, Eval=43.0\n",
            "Online Episode 132, Return=75.0, Eval=93.5\n",
            "Online Episode 134, Return=50.0, Eval=80.5\n",
            "Online Episode 136, Return=62.0, Eval=82.0\n",
            "Online Episode 138, Return=60.0, Eval=46.5\n",
            "Online Episode 140, Return=69.0, Eval=54.5\n",
            "Online Episode 142, Return=53.0, Eval=53.0\n",
            "Online Episode 144, Return=64.0, Eval=63.0\n",
            "Online Episode 146, Return=64.0, Eval=73.0\n",
            "Online Episode 148, Return=98.0, Eval=113.0\n",
            "Online Episode 150, Return=102.0, Eval=110.5\n",
            "Online Episode 152, Return=155.0, Eval=105.5\n",
            "Online Episode 154, Return=83.0, Eval=83.5\n",
            "Online Episode 156, Return=79.0, Eval=94.0\n",
            "Online Episode 158, Return=90.0, Eval=87.0\n",
            "Online Episode 160, Return=112.0, Eval=98.0\n",
            "Online Episode 162, Return=125.0, Eval=84.5\n",
            "Online Episode 164, Return=81.0, Eval=70.5\n",
            "Online Episode 166, Return=64.0, Eval=70.5\n",
            "Online Episode 168, Return=66.0, Eval=63.5\n",
            "Online Episode 170, Return=64.0, Eval=66.0\n",
            "Online Episode 172, Return=68.0, Eval=68.0\n",
            "Online Episode 174, Return=74.0, Eval=74.5\n",
            "Online Episode 176, Return=78.0, Eval=83.5\n",
            "Online Episode 178, Return=103.0, Eval=103.5\n",
            "Online Episode 180, Return=240.0, Eval=226.5\n",
            "Online Episode 182, Return=192.0, Eval=193.5\n",
            "Online Episode 184, Return=278.0, Eval=203.5\n",
            "Online Episode 186, Return=155.0, Eval=230.5\n",
            "Online Episode 188, Return=141.0, Eval=207.0\n",
            "Online Episode 190, Return=139.0, Eval=166.0\n",
            "Online Episode 192, Return=143.0, Eval=132.0\n",
            "Online Episode 194, Return=136.0, Eval=157.5\n",
            "Online Episode 196, Return=171.0, Eval=260.0\n",
            "Online Episode 198, Return=150.0, Eval=178.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(agent)"
      ],
      "metadata": {
        "id": "5H_cpxA9nqi4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "037eb493-5546-4fc6-c365-d8b85a3418cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x17 and 4x256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cec2c7de50c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-5afd5b23ac7d>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-571e43834cad>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, deterministic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Convert to Tensor on self.device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0maction_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-571e43834cad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Returns actions in [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# CQL Agent (twin Q-networks, random+policy actions in CQL penalty)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x17 and 4x256)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EVhOjphAJRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}